{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lowest level of capability for including the P-S cell relation, skip otherwise:\n",
    "CAPABILITY_LIMIT = 0.1\n",
    "\n",
    "# Set max number of BB partners per direction:\n",
    "MAX_BB_PARTNERS = 6\n",
    "\n",
    "# Set max number of external cells per partner and per direction:\n",
    "MAX_EXTERNAL_CELLS_SECONDARY_GNB = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input from Flow Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectivity = 'full' # full or partial - decision from the customer\n",
    "node_list = [] # List of nodes to exclude from the total\n",
    "unwanted_bb_links = [] #List of links not to create\n",
    "mandatory_bb_links = [] # List of links that must be created\n",
    "\n",
    "def set_node_selection(selection, primary_nodes, unwanted_links, mandatory_links):\n",
    "    selectivity = selection\n",
    "    node_list = primary_nodes\n",
    "    unwanted_bb_links = unwanted_links\n",
    "    mandatory_bb_links = mandatory_links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load topology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_topology_data(filename):\n",
    "    return open_file(filename)\n",
    "\n",
    "def get_TMO_topology_data(source_dir):\n",
    "    df = fetch_topology_data(source_dir + '/cucp_cellcu_merge.csv')\n",
    "    df['cell_fdn'] = df['meFdn'] + ',' + df['refId']\n",
    "    df.rename({'CellName': 'nRCellCUId', 'meFdn': 'node_fdn',\n",
    "                       'GNBID': 'gNBId', 'GNBID_LENGTH': 'gNBIdLength'}, axis=1, inplace=True)\n",
    "    df.drop(['parentRefId', 'refId', 'cellLocalId', 'MCC', 'MNC'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_SwissCom_topology_data(source_dir):\n",
    "    df_NRCellCU = fetch_topology_data(source_dir + '/NRCellCU.csv')\n",
    "    df_NRCellCU['NodeName'] = df_NRCellCU['fdn'].apply(lambda x: split_name(x, 'ManagedElement='))\n",
    "    df_NRCellCU.drop(['nRFrequencyRef'], axis=1, inplace=True)\n",
    "    df_NRCellCU.rename({'fdn': 'cell_fdn'}, axis=1, inplace=True)\n",
    "    \n",
    "    df_GNBCUCPFunction = fetch_topology_data(source_dir + '/GNBCUCPFunction.csv')\n",
    "    df_GNBCUCPFunction['NodeName'] = df_GNBCUCPFunction['fdn'].apply(lambda x: split_name(x, 'ManagedElement='))\n",
    "    df_GNBCUCPFunction.rename({'fdn': 'node_fdn'}, axis=1, inplace=True)\n",
    "    \n",
    "    df = df_NRCellCU.merge(df_GNBCUCPFunction, left_on='NodeName', right_on='NodeName')    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger MO Action for nodes of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_to_evaluate(selectivity, node_list, df_topology):\n",
    "    df = pd.DataFrame()\n",
    "    if selectivity == 'full':\n",
    "        df = df_topology[ ~df_topology['NodeName'].isin(node_list) ]\n",
    "    elif selectivity == 'partial':\n",
    "        df = df_topology[ df_topology['NodeName'].isin(node_list) ]\n",
    "    return df\n",
    "\n",
    "def trigger_mo_action(df_evaluation):\n",
    "    count = 0    \n",
    "    for index, row in df_evaluation.iterrows():\n",
    "        # The MO action would be triggered through NCMP per cell\n",
    "        # The node and cell details would also be pushed to the PM Handler to filter incoming data so we only consume what we are interested in. \n",
    "        # print ('MO action triggered for cell', row['nRCellCUId'], 'on node', row['NodeName'])\n",
    "        count+=1\n",
    "    \n",
    "    return 1, count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Coverage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_coverage_data(source_dir, date):\n",
    "    return open_file(source_dir + '/coverage_data_' + date +'.csv')\n",
    "\n",
    "def decode_hitrate(arcBalance):\n",
    "     return arcBalance\n",
    "# def decoded_usefulness(coded_usefulness):\n",
    "#     # Sets the constant use (basically only a scaling):\n",
    "#     k = 0.2\n",
    "#     return (coded_usefulness / k) ** (1 / (k - 1))\n",
    "\n",
    "def convert_hitrate(df):\n",
    "    df['hitrate'] = [\n",
    "        decode_hitrate(val)\n",
    "        for val in df['arcBalance']\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def get_coverage_data(source_dir, date, df_top_enrich):\n",
    "    # Fetch the data from the PM Handler (csv in this case)\n",
    "    df = fetch_coverage_data(source_dir, date)\n",
    "    \n",
    "    # Enrich the coverage data with the node and cell info from the topology\n",
    "    df = df_top_enrich.merge(df, right_on=['src_nCI'], left_on=['nCI'])\n",
    "    df.rename({'gNBId': 'src_gNBId'}, axis=1, inplace=True)\n",
    "    df.drop(['src_nRFrequencyRef', 'tgt_nRFrequencyRef', 'nCI'], axis=1, inplace=True)\n",
    "    \n",
    "     # Filter out rows where the source and target nodes are the same. \n",
    "    df = (\n",
    "        df[ df[\"src_gNBId\"] != df[\"tgt_gNBId\"] ]\n",
    "    ).copy()\n",
    "    \n",
    "    # Decode the hitrate\n",
    "    if not df.empty:\n",
    "        df = convert_hitrate(df)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Capacity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_capacity_data(df_required_ids, source_dir, date):\n",
    "    df = open_file(source_dir + '/capacity_data_' + date + '.csv')\n",
    "    # Pull the node name and cell name \n",
    "    df['NodeName'] = df['fdn'].apply(lambda x: split_name(x, 'ManagedElement='))\n",
    "    df['CellName'] = df['fdn'].apply(lambda x: split_name(x, 'NRCellDU='))\n",
    "    \n",
    "    # Filter for the data needed\n",
    "    df = df.merge(df_required_ids, left_on=['NodeName','CellName'], right_on=['NodeName', 'nRCellCUId'])\n",
    "    \n",
    "    # Drop the columns we dont need\n",
    "    df.drop(['fdn', 'timestamp', 'NodeName', 'nRCellCUId', 'CellName'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_kpis(df_pm):\n",
    "    df_pm[\"RBSymFree\"] = (\n",
    "        df_pm[\"pmMacRBSymAvailDl\"] - df_pm[\"pmMacRBSymUsedPdschTypeA\"] )\n",
    "    return df_pm\n",
    "\n",
    "\n",
    "def get_model(file_name):\n",
    "    df = pd.read_csv(file_name, sep=',')\n",
    "    return df.loc[0][\"intercept\"], df.loc[0][\"RBSymFree\"], df.loc[1][\"RBSymFree\"]\n",
    "\n",
    "def normalize_data(df, normalize_columns, normalize_constants):\n",
    "    for index, col in enumerate(normalize_columns):\n",
    "        df[col + \"Norm\"] = [\n",
    "            (value / normalize_constants[index]) for value in df[col]\n",
    "        ]\n",
    "    return df\n",
    "\n",
    "def add_predicted_cell_capacity(model_file_name, df_capacity_data):\n",
    "    intercept, coefficient_0, normalize_constant_0 = get_model( model_file_name )\n",
    "    df_capacity_pred = normalize_data(\n",
    "        df_capacity_data, [\"RBSymFree\"], [normalize_constant_0]\n",
    "    )\n",
    "\n",
    "    df_capacity_pred[\"predictedCapacity\"] = (\n",
    "        intercept + coefficient_0 * df_capacity_pred[\"RBSymFreeNorm\"]\n",
    "    )\n",
    "    return df_capacity_pred\n",
    "\n",
    "\n",
    "def get_capacity_data(df_required_ids, df_top_enrich, source_dir, date):\n",
    "    df = fetch_capacity_data(df_required_ids, source_dir, date)\n",
    "    df = calculate_kpis(df)\n",
    "    df = add_predicted_cell_capacity('modelParametersLinRegr_60.csv', df)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Usability Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_usability_matrix(coverage_data, capacity_data, capability_limit):\n",
    "    usability_matrix = coverage_data.copy()\n",
    "    # Get predicted capacity for matching sNCI\n",
    "    usability_matrix = capacity_data.merge(coverage_data, how=\"left\", on=\"tgt_nCI\").drop_duplicates()\n",
    "    # Change NaN to 0, if any\n",
    "    usability_matrix[\"predictedCapacity\"].fillna(0, inplace=True)\n",
    "    # Multiply hit rate with predicted capacity\n",
    "    usability_matrix[\"usability\"] = (\n",
    "        usability_matrix[\"hitrate\"] * usability_matrix[\"predictedCapacity\"]\n",
    "    )\n",
    "    \n",
    "    usability_matrix[\"usability\"].where(\n",
    "        usability_matrix[\"usability\"] > capability_limit, other=0, inplace=True\n",
    "    )\n",
    "    \n",
    "     # Clean up the resulting data frame\n",
    "    usability_matrix.rename({'src_gNBId': 'primary_gnb', \n",
    "                         'tgt_gNBId_x': 'secondary_gnb',\n",
    "                        'src_nCI': 'pNCI',\n",
    "                        'tgt_nCI': 'sNCI'}, axis=1, inplace=True)\n",
    "\n",
    "    usability_matrix.drop(\n",
    "        columns=[\n",
    "            \"src_nRCellCUId\",\n",
    "            \"tgt_nRCellCUId\",\n",
    "            #\"tgt_pLMNId\",\n",
    "            \"tgt_gNBIdLength\",\n",
    "            \"arcBalance\",\n",
    "            #\"hitrate\",\n",
    "            \"tgt_gNBId_y\",\n",
    "            \"pmMacRBSymUsedPdschTypeA\",\n",
    "            \"pmMacRBSymAvailDl\",\n",
    "            \"RBSymFree\",\n",
    "            \"RBSymFreeNorm\",\n",
    "            \"predictedCapacity\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )    \n",
    "    return usability_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create link value list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_link_value_list(usability_matrix):\n",
    "    # Remove zero usability rows:\n",
    "    bb_link_value_list = usability_matrix[usability_matrix[\"usability\"] > 0].copy()\n",
    "    # Add tuple with (primary_gnb, secondary_gnb):\n",
    "    bb_link_value_list[\"gNbs\"] = list(\n",
    "        zip(bb_link_value_list[\"primary_gnb\"], bb_link_value_list[\"secondary_gnb\"])\n",
    "    )\n",
    "    # Sort the entire data frame based on the columns of gNBs and the usability\n",
    "    bb_link_value_list.sort_values(\n",
    "        [\"primary_gnb\", \"secondary_gnb\", \"usability\"], inplace=True, ascending=False\n",
    "    )\n",
    "    # Aggregate usability value per eNB pair and sort the list in descending usability order\n",
    "    bb_link_value_list = (\n",
    "        bb_link_value_list[[\"primary_gnb\", \"secondary_gnb\", \"gNbs\", \"usability\", \"hitrate\"]]\n",
    "        .groupby([\"primary_gnb\", \"secondary_gnb\", \"gNbs\"])\n",
    "        .agg({\"usability\": \"sum\", \"hitrate\": \"mean\"})\n",
    "        .sort_values(\"usability\", ascending=False)\n",
    "    )\n",
    "    # Set numerical index\n",
    "    bb_link_value_list.reset_index(inplace=True)\n",
    "    # Add column indicating if link is used in configuration\n",
    "    bb_link_value_list[\"selected\"] = False\n",
    "    \n",
    "    return bb_link_value_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark Unwanted links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_unwanted_bb_link(df_link_value, unwanted_bb_links, df_topology):\n",
    "    df_link_value['unwanted'] = False\n",
    "    unwanted_links_gnbids = []\n",
    "    for unwanted_bb_link in unwanted_bb_links:\n",
    "        bb_0_id = df_topology[df_topology['NodeName'] == unwanted_bb_link[0]]['gNBId'].values[0]\n",
    "        bb_1_id = df_topology[df_topology['NodeName'] == unwanted_bb_link[1]]['gNBId'].values[0]\n",
    "        unwanted_links_gnbids.append((bb_0_id, bb_1_id))\n",
    "    \n",
    "    \n",
    "    df_link_value.loc[ df_link_value.gNbs.isin(unwanted_links_gnbids), 'unwanted'] = True\n",
    "\n",
    "    df_link_value.reset_index(drop=True, inplace=True)\n",
    "    return df_link_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark Mandatory links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_mandatory_bb_links(bb_link_value_list, mandatory_bb_links, df_topology):\n",
    "    bb_link_value_list['mandatory'] = False\n",
    "    for bb in mandatory_bb_links:\n",
    "        bb_0_id = df_topology[df_topology['NodeName'] == bb[0]]['gNBId'].values[0]\n",
    "        bb_1_id = df_topology[df_topology['NodeName'] == bb[1]]['gNBId'].values[0]\n",
    "        bb = (bb_0_id, bb_1_id)\n",
    "#         unwanted = bb_link_value_list[bb_link_value_list[\"gNbs\"] == bb]['Unwanted']\n",
    "#         print(unwanted)\n",
    "        if bb in set(bb_link_value_list[\"gNbs\"]):\n",
    "            bb_link_value_list.loc[bb_link_value_list[\"gNbs\"] == bb, \"selected\"] = True\n",
    "            bb_link_value_list.loc[bb_link_value_list[\"gNbs\"] == bb, \"mandatory\"] = True\n",
    "        else:\n",
    "            bb_link_value_list.loc[len(bb_link_value_list.index)] = [\n",
    "                bb,\n",
    "                0,\n",
    "                bb[0],\n",
    "                bb[1],\n",
    "                True,\n",
    "                False,\n",
    "                True\n",
    "            ]\n",
    "    sorted_bb_link = bb_link_value_list.sort_values(\n",
    "        [\"selected\", \"usability\"], ascending=False\n",
    "    )\n",
    "    sorted_bb_link.reset_index(drop=True, inplace=True)\n",
    "    return sorted_bb_link"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Best links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_gnb_list(bb_link_value_list):\n",
    "    if bb_link_value_list.empty:\n",
    "        unique_gnbs = []\n",
    "    else:\n",
    "        unique_gnbs = sorted(\n",
    "            list(set(bb_link_value_list[\"gNbs\"].apply(lambda x: [x[0], x[1]]).sum()))\n",
    "        )\n",
    "    return unique_gnbs\n",
    "\n",
    "def assign_bb_links_inner(gnbs, gnb0, gnb1, unwanted, max_bb_partners):\n",
    "    # Create arrays initialized with zero links per gNB\n",
    "    links_per_gnb_to_sec_col = np.zeros(len(gnbs))\n",
    "    links_per_gnb_to_prim_col = np.zeros(len(gnbs))\n",
    "    # Create array initialized to False for every link usage\n",
    "    link_used_col = np.full(len(gnb0), False)\n",
    "    # Go through the gNB pairs (= link) and see if it is assignable\n",
    "    for link in range(len(link_used_col)):\n",
    "        # Check if gNb0 and gNb1 both has fewer links than allowed limit\n",
    "        index_0 = np.where(gnbs == gnb0[link])[0]\n",
    "        index_1 = np.where(gnbs == gnb1[link])[0]\n",
    "        unwanted_state = unwanted[link]\n",
    "        if (links_per_gnb_to_sec_col[index_0][0] < max_bb_partners) and (\n",
    "            links_per_gnb_to_prim_col[index_1][0] < max_bb_partners):\n",
    "            \n",
    "            if unwanted_state == False:\n",
    "                # Both gNBs have available links, set link to be used and increment number of links\n",
    "                link_used_col[link] = True\n",
    "                links_per_gnb_to_sec_col[index_0] += 1\n",
    "                links_per_gnb_to_prim_col[index_1] += 1\n",
    "    return link_used_col\n",
    "\n",
    "\n",
    "def assign_bb_links(bb_link_value_list, max_bb_partners):\n",
    "    unique_gnbs = get_unique_gnb_list(bb_link_value_list)\n",
    "    bb_link_value_list.loc[:, \"selected\"] = assign_bb_links_inner(\n",
    "        np.array(unique_gnbs),\n",
    "        bb_link_value_list[\"primary_gnb\"].values,\n",
    "        bb_link_value_list[\"secondary_gnb\"].values,\n",
    "        bb_link_value_list[\"unwanted\"].values,\n",
    "        max_bb_partners,\n",
    "    ).tolist()\n",
    "    return bb_link_value_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich selected links with node names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_link_selection(bb_link_resulted_df, df_topology):\n",
    "    df_node_names_mapping = df_topology[['NodeName', 'gNBId']].drop_duplicates()\n",
    "\n",
    "    df_links_node_names = bb_link_resulted_df.merge(df_node_names_mapping, how=\"inner\",\n",
    "                                    right_on=['gNBId'],\n",
    "                                    left_on=['primary_gnb'])\n",
    "\n",
    "    df_links_node_names = df_links_node_names.merge(df_node_names_mapping, how=\"inner\",\n",
    "                                    right_on=['gNBId'],\n",
    "                                    left_on=['secondary_gnb'])\n",
    "\n",
    "    df_links_node_names.rename({'NodeName_x': 'primary_node_name', \n",
    "                            'NodeName_y': 'secondary_node_name'}, axis=1, inplace=True)\n",
    "\n",
    "    df_links_node_names.drop(columns=[\"gNBId_x\", \"gNBId_y\"], inplace=True)\n",
    "\n",
    "    return df_links_node_names\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Main\" method calling pulling the algorithm together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm(source_dir, date):\n",
    "    # Load the topology data\n",
    "    df_topology = get_TMO_topology_data(source_dir)\n",
    "    #df_topology = get_SwissCom_topology_data(source_dir)\n",
    "\n",
    "\n",
    "    # Trigger the MO actions for the nodes we are interested in\n",
    "    df_evaluation = get_nodes_to_evaluate(selectivity, node_list, df_topology)\n",
    "    state, count = trigger_mo_action(df_evaluation)\n",
    "    if state:\n",
    "        print(str(count) + ' MO Actions triggered')\n",
    "    else:\n",
    "        print('MO Action trigger failed')\n",
    "\n",
    "\n",
    "    # Load the coverage data\n",
    "    df_coverage_data = get_coverage_data(source_dir, date, df_evaluation[['nCI', 'gNBId']])\n",
    "\n",
    "\n",
    "    # Load Capacity data for the secondary nodes only. \n",
    "    # We only need data for the target cells so we can filter based on the coverage data\n",
    "    df_required_ids = df_coverage_data[['tgt_nCI', 'tgt_gNBId']].drop_duplicates()\n",
    "    # Enrich the required data using topology so we can use it to filter the incoming PM data\n",
    "    df_top_enrich = df_topology[['nCI', 'nRCellCUId', 'NodeName', 'gNBId']]\n",
    "    df_required_ids = df_required_ids.merge(df_top_enrich,\n",
    "                                    left_on=['tgt_gNBId','tgt_nCI'],\n",
    "                                    right_on=['gNBId', 'nCI'])\n",
    "    df_required_ids.drop(['gNBId', 'nCI'], axis=1, inplace=True)\n",
    "    df_capacity_data = get_capacity_data(df_required_ids, df_top_enrich, source_dir, date)\n",
    "    \n",
    "\n",
    "    # Use the data to build the usabililty matrix\n",
    "    df_usability_matrix = build_usability_matrix(df_coverage_data, df_capacity_data, CAPABILITY_LIMIT)\n",
    "\n",
    "\n",
    "    # Create link value list\n",
    "    df_link_value = create_link_value_list(df_usability_matrix)\n",
    "\n",
    "\n",
    "    # Mark unwanted links\n",
    "    df_link_value = mark_unwanted_bb_link( df_link_value, unwanted_bb_links, df_topology )\n",
    "\n",
    "\n",
    "    # Mark Mandatory links\n",
    "    df_link_value = mark_mandatory_bb_links( df_link_value, mandatory_bb_links, df_topology )\n",
    "    \n",
    "    \n",
    "    # Select the priority links\n",
    "    bb_link_resulted_df = assign_bb_links(df_link_value, MAX_BB_PARTNERS )\n",
    "\n",
    "\n",
    "    # Enrich the selected links with node names\n",
    "    df_links_node_names = enrich_link_selection(bb_link_resulted_df, df_topology)\n",
    "    \n",
    "    return df_links_node_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
